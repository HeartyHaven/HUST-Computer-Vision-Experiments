{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用keras架构实现MNIST两位数字比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 主要工作\n",
    "该实验相对之前两个实验的特殊性在于需要自己想办法构建一个mnist数字比较数据集。因此第一步是将数据集构建出来，这里不必保存到本地。具体来讲，思路是：\n",
    "\n",
    "1. 挑选61000条二分类样本(这个规模已经够大了)， 我们让模型不重复地随机生成61000个彼此不同的下标二元组，作为数据索引。\n",
    "2. 解包这些二元组形成两个索引列表，查找x_train中对应的内容：x1_train,x2_train，这些共60000条；剩下的1000条作为x1_test和x2_test。\n",
    "3. 生成label.先得到y1和y2，然后统一比大小得到二分类标签0或者1。最终得到y_train,y_test。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "assert x_train.shape == (60000, 28, 28)\n",
    "assert x_test.shape == (10000, 28, 28)\n",
    "assert y_train.shape == (60000,)\n",
    "assert y_test.shape == (10000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随便显示一个图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设mnist_sample是一个(28, 28)的数组\n",
    "mnist_sample = x_train[2]\n",
    "mnist_label=y_train[2]\n",
    "plt.imshow(mnist_sample, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(mnist_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成并划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def generate_index(n, range_start, range_end,y):\n",
    "    flag=0\n",
    "    if n > (range_end - range_start + 1) ** 2:\n",
    "        raise ValueError(\"no enough indexs\")\n",
    "\n",
    "    unique_numbers = set()\n",
    "    while len(unique_numbers) < n:\n",
    "        num1 = random.randint(range_start, range_end)\n",
    "        num2 = random.randint(range_start, range_end)\n",
    "        if flag==0 and y[num1]==y[num2]:\n",
    "            continue\n",
    "        elif flag==1 and y[num1]!=y[num2]:\n",
    "            continue\n",
    "        unique_numbers.add((num1, num2))\n",
    "        flag=not flag\n",
    "    x1,x2=[list(t) for t in zip(*unique_numbers)]\n",
    "    return x1,x2\n",
    "\n",
    "def make_dataset(x_train,y_train,x_test,y_test):\n",
    "    x1_train_idx,x2_train_idx = generate_index(100000, 0, 6000,y_train[:6000])\n",
    "    x1_test_idx,x2_test_idx=generate_index(10000,0,1000,y_test[:1000])\n",
    "    \n",
    "    x1_bi_train=np.stack(np.take(x_train,x1_train_idx,axis=0),axis=0)\n",
    "    x2_bi_train=np.stack(np.take(x_train,x2_train_idx,axis=0),axis=0)\n",
    "    x_bi_train=np.stack((x1_bi_train,x2_bi_train),axis=-1)\n",
    "    y1_bi_train=np.stack(np.take(y_train,x1_train_idx,axis=0),axis=0)\n",
    "    y2_bi_train=np.stack(np.take(y_train,x2_train_idx,axis=0),axis=0)\n",
    "    y_bi_train=np.array([[0,1] if (a == b) else [1,0] for a, b in zip(y1_bi_train, y2_bi_train)])\n",
    "\n",
    "    x1_bi_test=np.stack(np.take(x_test,x1_test_idx,axis=0),axis=0)\n",
    "    x2_bi_test=np.stack(np.take(x_test,x2_test_idx,axis=0),axis=0)\n",
    "    x_bi_test=np.stack((x1_bi_test,x2_bi_test),axis=-1)\n",
    "    y1_bi_test=np.stack(np.take(y_test,x1_test_idx,axis=0),axis=0)\n",
    "    y2_bi_test=np.stack(np.take(y_test,x2_test_idx,axis=0),axis=0)\n",
    "    y_bi_test=np.array([[0,1] if (a == b) else [1,0] for a, b in zip(y1_bi_test, y2_bi_test)])\n",
    "    \n",
    "    print('Balanced dataset\\nTrain: {} : {},shape: {} \\nTest: {} : {},shape: {}'.format(y_bi_train[:,1].sum(),len(y_bi_train),x_bi_train.shape,y_bi_test[:,1].sum(),len(y_bi_test),x_bi_test.shape))\n",
    "    \n",
    "    return (x_bi_train,y_bi_train),(x_bi_test,y_bi_test)\n",
    "\n",
    "training_set,testing_set=make_dataset(x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "\n",
    "def residual_blocks(num_filter,input):\n",
    "    conv1 = Conv2D(filters=num_filter, kernel_size=(2, 2), strides=1, padding='same')(input)\n",
    "    bn1 = BatchNormalization(axis=-1)(conv1)\n",
    "    conv2 = Conv2D(filters=num_filter, kernel_size=(2, 2), strides=1, padding='same')(bn1)\n",
    "    bn2 = BatchNormalization(axis=-1)(conv2)\n",
    "    conv3 = Conv2D(filters=num_filter, kernel_size=(2, 2), strides=1, padding='same')(bn2)\n",
    "    bn3 = BatchNormalization(axis=-1)(conv3)\n",
    "    res=concatenate([input,bn3],axis=-1)\n",
    "    return res\n",
    "    # return bn3\n",
    "\n",
    "def feature_extraction(input_shape=(28,28,2)):\n",
    "    X_input = Input(input_shape)\n",
    "    #0填充\n",
    "    X = ZeroPadding2D((2,2))(X_input)\n",
    "\n",
    "    res1=residual_blocks(num_filter=6,input=X)\n",
    "\n",
    "    pool2=MaxPooling2D(strides=2)(res1)\n",
    "    res2=residual_blocks(num_filter=16,input=pool2)\n",
    "\n",
    "    pool3=MaxPool2D(strides=2)(res2)\n",
    "    out=Flatten()(pool3)\n",
    "    model=keras.Model(inputs=X_input,outputs=out,name='feature_extraction')\n",
    "    return model\n",
    "\n",
    "def classifier_10(input_shape=(1536,)):\n",
    "    X_input=Input(input_shape)\n",
    "    dense1=Dense(64,activation='relu')(X_input)\n",
    "    out=Dense(10,activation='softmax')(dense1)\n",
    "    model=keras.Model(inputs=X_input,outputs=out,name='10_head_classifier')\n",
    "    return model\n",
    "\n",
    "def classifier_2(input_shape=(1536,)):\n",
    "    X_input=Input(input_shape)\n",
    "    dense1=Dense(256,activation='relu')(X_input)\n",
    "    dense2=Dense(32,activation='relu')(dense1)\n",
    "    out=Dense(2,activation='softmax')(dense2)\n",
    "    model=keras.Model(inputs=X_input,outputs=out,name='2_head_classifier')\n",
    "    return model\n",
    "\n",
    "Feature_Extraction=feature_extraction()\n",
    "Cls_10=classifier_10()\n",
    "Cls_2=classifier_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用迁移学习：首先是单个数字识别模型的训练，然后封锁线性层转而训练二分类的分类头。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10=keras.Sequential()\n",
    "model_10.add(Feature_Extraction)\n",
    "model_10.add(Cls_10)\n",
    "model_10.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train 10-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_10_train=to_categorical(y_train,num_classes=10)\n",
    "x_10_train=np.stack((x_train,x_train),axis=-1)\n",
    "model_10.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_10.fit(x_10_train,y_10_train,\n",
    "             epochs=2,\n",
    "             batch_size=64,\n",
    "             verbose=1)\n",
    "model_10.layers[0].save('feature_extraction.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10.layers[1].save('10_head_classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train header of 2-head-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model_2=keras.Sequential()\n",
    "Pretrained_Feature_Extraction=load_model(r'.\\feature_extraction.h5',\n",
    "                                         custom_objects=None, \n",
    "                                         compile=True, \n",
    "                                         options=None,\n",
    "                                         )#报错是因为绝对路径中有中文名称\n",
    "model_2.add(Pretrained_Feature_Extraction)\n",
    "model_2.add(Cls_2)\n",
    "model_2.layers[0].trainable=False#freeze params\n",
    "model_2.layers[1].trainable=True\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def __init__(self,batchsize):\n",
    "        super().__init__()\n",
    "        self.losses = []\n",
    "        self.accs=[]\n",
    "        self.val_losses=[]\n",
    "        self.val_accs=[]\n",
    "        self.x_test = testing_set[0]\n",
    "        self.y_test = testing_set[1]\n",
    "        # print(batch)\n",
    "        self.batch_size = batchsize\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        indices = random.sample(range(10000), self.batch_size)\n",
    "        x_batch = self.x_test[indices]\n",
    "        y_batch = self.y_test[indices]\n",
    "        # print(y_batch.shape)\n",
    "        val_loss, val_acc = self.model.evaluate(x_batch, y_batch, verbose=0)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.accs.append(logs.get('accuracy'))\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.val_accs.append(val_acc)\n",
    "\n",
    "\n",
    "model_2.compile(optimizer='Adam', loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "bs=64\n",
    "callback = LossHistory(batchsize=bs)\n",
    "# 训练模型\n",
    "history_small = model_2.fit(\n",
    "    training_set[0], training_set[1],\n",
    "    epochs=3,\n",
    "    batch_size=bs,\n",
    "    verbose=1,\n",
    "    callbacks=[callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "updating_steps_loss = range(20,len(callback.losses))\n",
    "updating_steps = range(len(callback.losses))\n",
    "\n",
    "\n",
    "# 绘制图表\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# losses和val_losses\n",
    "plt.subplot(221)\n",
    "plt.plot(updating_steps_loss, callback.losses[20:], label='training_loss', color='blue')\n",
    "# plt.xlabel('updating steps')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "# accs和val_accs\n",
    "plt.subplot(222)\n",
    "plt.plot(updating_steps,callback.accs, label='training_accuracy', color='blue')\n",
    "# plt.xlabel('updating steps')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# losses和val_losses\n",
    "plt.subplot(223)\n",
    "plt.plot(updating_steps_loss, callback.val_losses[20:], label='validation_loss', color='orange')\n",
    "plt.xlabel('updating steps')\n",
    "plt.ylabel('validation loss')\n",
    "plt.legend()\n",
    "\n",
    "# accs和val_accs\n",
    "plt.subplot(224)\n",
    "plt.plot(updating_steps,callback.val_accs, label='validation_accuracy', color='orange')\n",
    "plt.xlabel('updating steps')\n",
    "plt.ylabel('validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "# plt.savefig('output_bs128.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('output.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试集上的准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predictions=model_2.predict(testing_set[0],batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo=classification_report(testing_set[1][:,1],predictions.argmax(axis=-1),target_names=['not_same','same'])\n",
    "with open('bs128.txt','w')as f:\n",
    "    print('report:\\n',repo,file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    "def show_values(pc, fmt=\"%.2f\", **kw):\n",
    "    import numpy as np\n",
    "    pc.update_scalarmappable()\n",
    "    ax = pc.axes\n",
    "    for p, color, value in zip(pc.get_paths(), pc.get_facecolors(), pc.get_array()):\n",
    "        x, y = p.vertices[:-2, :].mean(0)\n",
    "        if np.all(color[:3] > 0.5):\n",
    "            color = (0.0, 0.0, 0.0)\n",
    "        else:\n",
    "            color = (1.0, 1.0, 1.0)\n",
    "        ax.text(x, y, fmt % value, ha=\"center\", va=\"center\", color=color, **kw)\n",
    "\n",
    " \n",
    " \n",
    "def cm2inch(*tupl):\n",
    "    '''\n",
    "    Specify figure size in centimeter in matplotlib\n",
    "    Source: https://stackoverflow.com/a/22787457/395857\n",
    "    By gns-ank\n",
    "    '''\n",
    "    inch = 2.54\n",
    "    if type(tupl[0]) == tuple:\n",
    "        return tuple(i/inch for i in tupl[0])\n",
    "    else:\n",
    "        return tuple(i/inch for i in tupl)\n",
    " \n",
    " \n",
    "def heatmap(AUC, title, xlabel, ylabel, xticklabels, yticklabels, figure_width=40, figure_height=20, correct_orientation=False, cmap='RdBu'):\n",
    "    '''\n",
    "    Inspired by:\n",
    "    - https://stackoverflow.com/a/16124677/395857 \n",
    "    - https://stackoverflow.com/a/25074150/395857\n",
    "    '''\n",
    " \n",
    "    # Plot it out\n",
    "    fig, ax = plt.subplots()    \n",
    "    #c = ax.pcolor(AUC, edgecolors='k', linestyle= 'dashed', linewidths=0.2, cmap='RdBu', vmin=0.0, vmax=1.0)\n",
    "    c = ax.pcolor(AUC, edgecolors='k', linestyle= 'dashed', linewidths=0.2, cmap=cmap)\n",
    " \n",
    "    # put the major ticks at the middle of each cell\n",
    "    ax.set_yticks(np.arange(AUC.shape[0]) + 0.5, minor=False)\n",
    "    ax.set_xticks(np.arange(AUC.shape[1]) + 0.5, minor=False)\n",
    " \n",
    "    # set tick labels\n",
    "    #ax.set_xticklabels(np.arange(1,AUC.shape[1]+1), minor=False)\n",
    "    ax.set_xticklabels(xticklabels, minor=False)\n",
    "    ax.set_yticklabels(yticklabels, minor=False)\n",
    " \n",
    "    # set title and x/y labels\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)      \n",
    " \n",
    "    # Remove last blank column\n",
    "    plt.xlim( (0, AUC.shape[1]) )\n",
    " \n",
    "    # Turn off all the ticks\n",
    "    ax = plt.gca()    \n",
    "    for t in ax.xaxis.get_major_ticks():\n",
    "        t.tick1On = False\n",
    "        t.tick2On = False\n",
    "    for t in ax.yaxis.get_major_ticks():\n",
    "        t.tick1On = False\n",
    "        t.tick2On = False\n",
    " \n",
    "    # Add color bar\n",
    "    plt.colorbar(c)\n",
    " \n",
    "    # Add text in each cell \n",
    "    show_values(c)\n",
    " \n",
    "    # Proper orientation (origin at the top left instead of bottom left)\n",
    "    if correct_orientation:\n",
    "        ax.invert_yaxis()\n",
    "        ax.xaxis.tick_top()       \n",
    " \n",
    "    # resize \n",
    "    fig = plt.gcf()\n",
    "    #fig.set_size_inches(cm2inch(40, 20))\n",
    "    #fig.set_size_inches(cm2inch(40*4, 20*4))\n",
    "    fig.set_size_inches(cm2inch(figure_width, figure_height))\n",
    " \n",
    " \n",
    " \n",
    "def plot_classification_report(classification_report, title='Classification report ', cmap='RdBu'):\n",
    "    '''\n",
    "    Plot scikit-learn classification report.\n",
    "    Extension based on https://stackoverflow.com/a/31689645/395857 \n",
    "    '''\n",
    "    lines = classification_report.split('\\n')\n",
    " \n",
    "    classes = []\n",
    "    plotMat = []\n",
    "    support = []\n",
    "    class_names = []\n",
    "    for line in lines[2 : (len(lines) - 4)]:\n",
    "        t = line.strip().split()\n",
    "        if len(t) < 2: continue\n",
    "        classes.append(t[0])\n",
    "        print(t)\n",
    "        # for x in t[1: len(t) - 1]:\n",
    "        #     print(x)\n",
    "        if 'avg' in t[1]:\n",
    "            v = [float(x) for x in t[2: len(t) - 1]]\n",
    "        else:\n",
    "            v = [float(x) for x in t[1: len(t) - 1]]\n",
    "        support.append(int(t[-1]))\n",
    "        class_names.append(t[0])\n",
    "        print(v)\n",
    "        plotMat.append(v)\n",
    " \n",
    "    print('plotMat: {0}'.format(plotMat))\n",
    "    print('support: {0}'.format(support))\n",
    " \n",
    "    xlabel = 'Metrics'\n",
    "    ylabel = 'Classes'\n",
    "    xticklabels = ['Precision', 'Recall', 'F1-score']\n",
    "    yticklabels = ['{0} ({1})'.format(class_names[idx], sup) for idx, sup  in enumerate(support)]\n",
    "    figure_width = 25\n",
    "    figure_height = len(class_names) + 7\n",
    "    correct_orientation = False\n",
    "    heatmap(np.array(plotMat), title, xlabel, ylabel, xticklabels, yticklabels, figure_width, figure_height, correct_orientation, cmap=cmap)\n",
    " \n",
    " \n",
    "def main():\n",
    "    sampleClassificationReport = repo\n",
    " \n",
    "    plot_classification_report(sampleClassificationReport)\n",
    "    plt.savefig('test_plot_classif_report.png', dpi=200, format='png', bbox_inches='tight')\n",
    "    plt.close()\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    #cProfile.run('main()') # if you want to do some profiling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
